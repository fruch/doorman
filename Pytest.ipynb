{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "# Python Test Runners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Python Test Runners\n",
    "\n",
    "\n",
    "* Why testing ?!?!?\n",
    "\n",
    "\n",
    "* Available Test Runners\n",
    "    * unittest\n",
    "    * nose\n",
    "    * py.test\n",
    "     \n",
    "   \n",
    "* Real life example: \n",
    "    * https://github.com/fruch/doorman/tree/master/tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unittest\n",
    "\n",
    "https://docs.python.org/2/library/unittest.html\n",
    "\n",
    "* Available as pyhton standard library\n",
    "\n",
    "\n",
    "* Based on java xUnit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unittest - basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FF.\n",
      "======================================================================\n",
      "FAIL: test_fail1 (__main__.MyTestCase)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-8d8f365a93cc>\", line 8, in test_fail1\n",
      "    self.assertEqual(0, 1)\n",
      "AssertionError: 0 != 1\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_fail2 (__main__.MyTestCase)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-8d8f365a93cc>\", line 12, in test_fail2\n",
      "    self.assertEquals(\"ABC\", \"ADC\")\n",
      "AssertionError: 'ABC' != 'ADC'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.004s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x427ce10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class MyTestCase(unittest.TestCase):\n",
    "    def test_pass(self):\n",
    "        self.assertEqual(0, 0)\n",
    "        \n",
    "    def test_fail1(self):\n",
    "        self.assertEqual(0, 1)\n",
    "        \n",
    "            \n",
    "    def test_fail2(self):\n",
    "        self.assertEquals(\"ABC\", \"ADC\")\n",
    "\n",
    "unittest.main(defaultTest=\"MyTestCase\", argv=[__name__,], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unittest - Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"font-size: 50%\">\n",
    "```\n",
    "FF.\n",
    "======================================================================\n",
    "FAIL: test_fail1 (__main__.MyTestCase)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"<ipython-input-4-8d8f365a93cc>\", line 8, in test_fail1\n",
    "    self.assertEqual(0, 1)\n",
    "AssertionError: 0 != 1\n",
    "\n",
    "======================================================================\n",
    "FAIL: test_fail2 (__main__.MyTestCase)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"<ipython-input-4-8d8f365a93cc>\", line 12, in test_fail2\n",
    "    self.assertEquals(\"ABC\", \"ADC\")\n",
    "AssertionError: 'ABC' != 'ADC'\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 3 tests in 0.004s\n",
    "\n",
    "FAILED (failures=2)\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unittest - per test setup/teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x6484278>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PerTestSetupTestCase(unittest.TestCase):\n",
    "    def setUp(self):        \n",
    "        self.db = \"My slow and expensive connection to the DB will go here\"\n",
    "\n",
    "    def tearDown(self):\n",
    "        del self.db\n",
    "        \n",
    "    def test_pass(self):\n",
    "        # do something with the DB\n",
    "        self.assertIsNotNone(self.db)\n",
    "        \n",
    "unittest.main(defaultTest=\"`PerTestSetupTestCase\", \n",
    "              argv=[__name__,], \n",
    "              exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unittest - per class setup/teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unittest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-11e34bfe4ee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mHeavyTestCase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munittest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTestCase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetUpClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"My slow and expensive connection to the DB will go here\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unittest' is not defined"
     ]
    }
   ],
   "source": [
    "class HeavyTestCase(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):        \n",
    "        cls.db = \"My slow and expensive connection to the DB will go here\"\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        del cls.db\n",
    "        \n",
    "    def test_pass(self):\n",
    "        # do something with the DB\n",
    "        self.assertIsNotNone(self.db)\n",
    "        \n",
    "unittest.main(defaultTest=\"HeavyTestCase\", \n",
    "              argv=[__name__,], \n",
    "              exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nose\n",
    "\n",
    "> nose extends unittest to make testing easier.\n",
    "\n",
    "* Better commandline options\n",
    "\n",
    "\n",
    "* Extendable via plugins \n",
    "\n",
    "\n",
    "* Eco system with plugins\n",
    "\n",
    "\n",
    "http://nose.readthedocs.io/en/latest/#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nose - plugin example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FF."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"fail\": [\"(Test(<__main__.TestCaseNose testMethod=test_1>), u'Traceback (most recent call last):\\\\n  File \\\"<ipython-input-3-03312d18a0e5>\\\", line 9, in test_1\\\\n    self.assertEquals(\\\"A\\\", \\\"B\\\")\\\\nAssertionError: \\\\'A\\\\' != \\\\'B\\\\'\\\\n')\", \"(Test(<__main__.TestCaseNose testMethod=test_2>), u'Traceback (most recent call last):\\\\n  File \\\"<ipython-input-3-03312d18a0e5>\\\", line 12, in test_2\\\\n    self.assertEqual(1, 0)\\\\nAssertionError: 1 != 0\\\\n')\"], \"skip\": [], \"run\": 3, \"error\": []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAIL: test_1 (__main__.TestCaseNose)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-03312d18a0e5>\", line 9, in test_1\n",
      "    self.assertEquals(\"A\", \"B\")\n",
      "AssertionError: 'A' != 'B'\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_2 (__main__.TestCaseNose)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-03312d18a0e5>\", line 12, in test_2\n",
      "    self.assertEqual(1, 0)\n",
      "AssertionError: 1 != 0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.028s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json ; import nose; from nose.plugins import Plugin\n",
    "\n",
    "class MyNosePlugin(Plugin):\n",
    "    name = 'mynoseplugin'\n",
    "\n",
    "    def configure(self, options, conf):\n",
    "        super(MyNosePlugin, self).configure(options, conf)\n",
    "        if not self.enabled: return None\n",
    "        \n",
    "    def finalize(self, result):\n",
    "        data = dict( run=result.testsRun, \n",
    "           fail=[ str(i) for i in result.failures ],\n",
    "           error=[ str(i) for i in result.errors ],\n",
    "           skip=[ str(i) for i in result.skipped ])\n",
    "        \n",
    "        print json.dumps(data) \n",
    "        # TODO: Sending it to my ELK / CI / DB to save the inforamtion\n",
    "        \n",
    "nose.run(defaultTest=\"__main__\", argv=[__name__,\"--with-mynoseplugin\"], \n",
    "         addplugins=[MyNosePlugin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# py.test\n",
    "\n",
    "> helps you write better programs\n",
    "\n",
    "http://doc.pytest.org/en/latest/\n",
    "\n",
    "* A mature full-featured Python testing tool\n",
    "\n",
    "* Provides easy no-boilerplate testing\n",
    "\n",
    "* Scales from simple unit to complex functional testing\n",
    "\n",
    "* Integrates with other testing methods and tools\n",
    "\n",
    "* Extensive plugin and customization system [list on pypi](https://pypi.python.org/pypi?%3Aaction=search&term=pytest-&submit=search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### py.test - basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 2.7.10, pytest-2.9.2, py-1.4.31, pluggy-0.3.1\n",
      "rootdir: C:\\Users\\ifruchte\\Projects\\doorman, inifile: tox.ini\n",
      "plugins: cov-2.3.1, flask-0.10.0, testinfra-1.4.1\n",
      "collected 3 items\n",
      "\n",
      "test_simple.py .FF\n",
      "=========================== short test summary info ===========================\n",
      "FAIL test_simple.py::test_fail1\n",
      "FAIL test_simple.py::test_fail2\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "_________________________________ test_fail1 __________________________________\n",
      "\n",
      "    def test_fail1():\n",
      ">       assert 0 == 1\n",
      "E       assert 0 == 1\n",
      "\n",
      "test_simple.py:6: AssertionError\n",
      "_________________________________ test_fail2 __________________________________\n",
      "\n",
      "    def test_fail2():\n",
      ">       assert \"ABC\" == \"ADC\"\n",
      "E       assert 'ABC' == 'ADC'\n",
      "E         - ABC\n",
      "E         + ADC\n",
      "\n",
      "test_simple.py:10: AssertionError\n",
      "===================== 2 failed, 1 passed in 0.10 seconds ======================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_simple.py\n",
    "import pytest\n",
    "\n",
    "def test_pass():\n",
    "    assert 0 == 0\n",
    "\n",
    "def test_fail1():\n",
    "    assert 0 == 1\n",
    "\n",
    "def test_fail2():\n",
    "    assert \"ABC\" == \"ADC\"\n",
    "\n",
    "pytest.main(\"test_simple.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### py.test - output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"font-size: 40%\">\n",
    "```\n",
    "============================= test session starts =============================\n",
    "platform win32 -- Python 2.7.10, pytest-2.9.2, py-1.4.31, pluggy-0.3.1\n",
    "\n",
    "test_simple.py .FF\n",
    "=========================== short test summary info ===========================\n",
    "FAIL test_simple.py::test_fail1\n",
    "FAIL test_simple.py::test_fail2\n",
    "\n",
    "================================== FAILURES ===================================\n",
    "_________________________________ test_fail1 __________________________________\n",
    "\n",
    "    def test_fail1():\n",
    ">       assert 0 == 1\n",
    "E       assert 0 == 1\n",
    "\n",
    "test_simple.py:6: AssertionError\n",
    "_________________________________ test_fail2 __________________________________\n",
    "\n",
    "    def test_fail2():\n",
    ">       assert \"ABC\" == \"ADC\"\n",
    "E       assert 'ABC' == 'ADC'\n",
    "E         - ABC\n",
    "E         + ADC\n",
    "\n",
    "test_simple.py:10: AssertionError\n",
    "===================== 2 failed, 1 passed in 0.10 seconds ======================\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### py.test - plugin example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 2.7.10, pytest-2.9.2, py-1.4.31, pluggy-0.3.1\n",
      "rootdir: C:\\Users\\ifruchte\\Projects\\doorman, inifile: tox.ini\n",
      "plugins: cov-2.3.1, flask-0.10.0, testinfra-1.4.1\n",
      "collected 18 items\n",
      "\n",
      "tests\\test_doorman.py ........s\n",
      "=========================== short test summary info ===========================\n",
      "SKIP [1] tests\\test_doorman.py:79: requires python3\n",
      "\n",
      "=================== 9 tests deselected by '-ktest_doorman' ====================\n",
      "============== 8 passed, 1 skipped, 9 deselected in 1.41 seconds ==============\n",
      "{'fail': [], 'skip': [], 'run': 8, 'error': []}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "class MyPlugin:\n",
    "    data = []\n",
    "        \n",
    "          \n",
    "    def pytest_report_teststatus(self, report):\n",
    "        # report.when in ['call', 'setup', 'teardown']\n",
    "        if report.when == \"call\":\n",
    "            self.data.append(dict(name=report.nodeid, \n",
    "                                  outcome=report.outcome))\n",
    "            \n",
    "    def pytest_unconfigure(self, config):\n",
    "        # error would come from setup/teardown, and we didn't \n",
    "        # collected them, or things like skip...\n",
    "        data = dict(\n",
    "           run=len(self.data),\n",
    "           fail=[ i for i in self.data if i['outcome'] == \"failed\" ], \n",
    "           error=[ i for i in self.data if i['outcome'] == \"error\" ], \n",
    "           skip=[ i for  i in self.data if i['outcome'] == \"skipped\" ],\n",
    "        )\n",
    "        print data\n",
    "        \n",
    "pytest.main(\"-k test_doorman\", plugins=[ MyPlugin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### py.test - fixtures - per test setup/teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 2.7.10, pytest-2.9.2, py-1.4.31, pluggy-0.3.1\n",
      "rootdir: C:\\Users\\ifruchte\\Projects\\doorman, inifile: tox.ini\n",
      "plugins: cov-2.3.1, flask-0.10.0, testinfra-1.4.1\n",
      "collected 1 items\n",
      "\n",
      "test_something.py F\n",
      "=========================== short test summary info ===========================\n",
      "FAIL test_something.py::test_something_with_db\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "___________________________ test_something_with_db ____________________________\n",
      "\n",
      "cheap_db = {'mykey': '12345', 'newket': '$$$'}\n",
      "\n",
      "    def test_something_with_db(cheap_db):\n",
      "        assert cheap_db['mykey'] == \"12345\"\n",
      "        cheap_db[\"newket\"] = \"$$$\"\n",
      "    \n",
      "    \n",
      ">       assert 1 < 0\n",
      "E       assert 1 < 0\n",
      "\n",
      "test_something.py:22: AssertionError\n",
      "========================== 1 failed in 0.07 seconds ===========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "# per test setup/teardown\n",
    "@pytest.fixture(scope=\"function\")\n",
    "def cheap_db(request):\n",
    "    \"\"\" In memory DB, cheap I can create for each test \"\"\"\n",
    "    db = dict(mykey=\"12345\")\n",
    "    \n",
    "    def fin():\n",
    "        print db # here we could clean it up\n",
    "        \n",
    "    request.addfinalizer(fin)\n",
    "    return db\n",
    "\n",
    "def test_something_with_db(cheap_db):    \n",
    "    assert cheap_db['mykey'] == \"12345\"\n",
    "    cheap_db[\"newket\"] = \"$$$\"\n",
    "    \n",
    "pytest.main(\"test_something.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### py.test - fixtures - per session setup/teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 2.7.10, pytest-2.9.2, py-1.4.31, pluggy-0.3.1\n",
      "rootdir: C:\\Users\\ifruchte\\Projects\\doorman, inifile: tox.ini\n",
      "plugins: cov-2.3.1, flask-0.10.0, testinfra-1.4.1\n",
      "collected 1 items\n",
      "\n",
      "test_something.py F\n",
      "=========================== short test summary info ===========================\n",
      "FAIL test_something.py::test_something_with_db\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "___________________________ test_something_with_db ____________________________\n",
      "\n",
      "cheap_db = {'mykey': '12345', 'newket': '$$$'}\n",
      "\n",
      "    def test_something_with_db(cheap_db):\n",
      "        assert cheap_db['mykey'] == \"12345\"\n",
      "        cheap_db[\"newket\"] = \"$$$\"\n",
      "    \n",
      "    \n",
      ">       assert 1 < 0\n",
      "E       assert 1 < 0\n",
      "\n",
      "test_something.py:22: AssertionError\n",
      "========================== 1 failed in 0.06 seconds ===========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "# per session setup/teardown\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def heavy_db(request):\n",
    "    db = dict(mykey=\"12345\")\n",
    "    def fin():\n",
    "        print db # here we could clean it up\n",
    "        \n",
    "    request.addfinalizer(fin)\n",
    "    return db\n",
    "\n",
    "def test_something_with_db(heavy_db):    \n",
    "    assert heavy_db['mykey'] == \"12345\"\n",
    "    heavy_db[\"newket\"] = \"$$$\"\n",
    "    \n",
    "pytest.main(\"test_something.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### py.test - [marking tests](http://doc.pytest.org/en/latest/mark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "@pytest.mark.slowtest\n",
    "def test_function():\n",
    "   pass\n",
    "\n",
    "@pytest.mark.best_feature\n",
    "def test_function():\n",
    "   pass\n",
    "\n",
    "# you can call it like:\n",
    "# $ py.test -m 'best_feature and not slowtest' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# skiping in py.test is also a kind of marking \n",
    "@pytest.mark.skipif(sys.version_info < (3,), reason=\"requires python3\")\n",
    "def test_something_python3_only():\n",
    "    import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Command-line usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Unittest - you need to be specifc\n",
    "    python -m unittest test_unittest\n",
    "\n",
    "### Nose - selection is easier, and you have plugings\n",
    "    nosetests\n",
    "    nosetests test_unittest.py --with-coverage\n",
    "\n",
    "### Py.test - selection is easy, outout is much much helpful\n",
    "    py.test tests/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Real life examples - Flask App Database mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "import mongomock\n",
    "\n",
    "import doorman\n",
    "from doorman.webserver import app as doorman_app\n",
    "\n",
    "# create the mock db for the test\n",
    "db = mongomock.MongoClient().db.resources\n",
    "db.ensure_index('id', unique=True)\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def app():\n",
    "    def get_resources_db_mock():\n",
    "        return db\n",
    "    \n",
    "    # patch the function that initiate the db\n",
    "    doorman.webserver.get_resources_db = get_resources_db_mock\n",
    "    \n",
    "    # setup the application for testing\n",
    "    doorman_app.config[\"DEBUG\"] = True\n",
    "    doorman_app.config[\"TESTING\"] = True\n",
    "    \n",
    "    return doorman_app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pytest\n",
    "from flask import url_for\n",
    "\n",
    "headers={\"Content-Type\": \"application/json\"}\n",
    "\n",
    "def test_put_resource(client):\n",
    "    assert client.put(url_for('lockedresource', id='box1'),\n",
    "                      headers=headers, data=json.dumps(dict())\n",
    "                      ).status_code == 200\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# snippet from pytest-flask\n",
    "# https://github.com/pytest-dev/pytest-flask/blob/\n",
    "# master/pytest_flask/fixtures.py\n",
    "\n",
    "@pytest.yield_fixture\n",
    "def client(app):\n",
    "    \"\"\"A Flask test client. An instance of :class:`flask.testing.\n",
    "    TestClient` by default.\n",
    "    \"\"\"\n",
    "    with app.test_client() as client:\n",
    "        yield client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Real life examples - starting docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pytest    TopLevelCommand, project_from_options\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def docker_compose(request):\n",
    "    \"\"\"\n",
    "    :type request: _pytest.python.FixtureRequest\n",
    "    \"\"\"\n",
    "\n",
    "    options = {\"--no-deps\": False,\n",
    "               # ... snip ... lot of defaults\n",
    "               \"-d\": True,\n",
    "               }\n",
    "\n",
    "    project = project_from_options(os.path.dirname(__file__), options)\n",
    "    cmd = TopLevelCommand(project)\n",
    "    cmd.up(options)\n",
    "\n",
    "    def fin():\n",
    "        cmd.logs(options)\n",
    "        cmd.down(options)\n",
    "\n",
    "    request.addfinalizer(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Real life examples - starting docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import wait\n",
    "import requests\n",
    "\n",
    "def test_integration(docker_compose):\n",
    "    wait.tcp.open(5000, host='localhost')\n",
    "\n",
    "    requests.delete(\"http://localhost:5000/resource/1\")\n",
    "    res = requests.put(\"http://localhost:5000/resource/1\", json=dict(name=\"moshe\", card_id=\"1234\"))\n",
    "    assert res.status_code == 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Real life examples - plotting fixutres dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pytest_runtest_setup(item):\n",
    "    curdir = py.path.local()\n",
    "    data = dict()\n",
    "    data['func_args'] = item._fixtureinfo.argnames, 'red'\n",
    "    \n",
    "    for fixture_name, fixture_data in item._fixtureinfo.name2fixturedefs.items():\n",
    "        loc = getlocation(fixture_data[0].func, curdir)\n",
    "        color = 'yellow' if 'pytest_vgw' in loc else 'green'\n",
    "        data[fixture_name] = fixture_data[0].argnames, color\n",
    "\n",
    "    graph = pydot.Dot(graph_type='digraph')\n",
    "    for name, depended_list in data.items():\n",
    "        depended_list, color = depended_lis\n",
    "        node = pydot.Node(name, style=\"filled\", fillcolor=color)\n",
    "        graph.add_node(node)\n",
    "        for i in depended_list:\n",
    "            edge = pydot.Edge(node, i)\n",
    "            graph.add_edge(edge)\n",
    "    log_dir = os.environ.get('LOG_DEST_DIR', 'artifacts')\n",
    "    mkdir_recursive(log_dir)\n",
    "    filename = \"%s/fixture-graph-%s\" % (log_dir, item._nodeid.replace(\":\", \"_\").replace(\"/\", \"-\"))\n",
    "    graph.write(filename + \".dot\")\n",
    "    logger.info(\"Fixture graph created at %s.dot. \"\n",
    "                \"You can convert it to a PNG using 'dot -Tpng %s.dot -o %s.png'\", \n",
    "                filename, filename, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Real life examples - plotting fixutres dependecies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![alt](http://rdkucs1.il.nds.com/~ifruchte/test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions\n",
    "</br>\n",
    "### israel.fruhcter@gmail.com \n",
    "</br>\n",
    "### http://fruch.github.io/\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
